org.apache.hadoop.hdfs.server.namenode.NameNode
edgeAnalysis for function get [java.lang.String]
edgeAnalysis for function get [java.lang.String, java.lang.String]
edgeAnalysis for function getInt
edgeAnalysis for function getInts
edgeAnalysis for function getLong
edgeAnalysis for function getLongBytes
edgeAnalysis for function getHexDigits
edgeAnalysis for function getFloat
edgeAnalysis for function getDouble
edgeAnalysis for function getBoolean
Results: parameters statically used on component org.apache.hadoop.hdfs.server.namenode.NameNode
60 parameters read from function getLong:
"dfs.disk.balancer.max.disk.throughputInMBperSec"
"dfs.balancer.max-iteration-time"
"dfs.client.cache.readahead"
"dfs.namenode.full.block.report.lease.length.ms"
"dfs.namenode.delegation.key.update-interval"
"dfs.namenode.resource.du.reserved"
"dfs.namenode.delegation.token.max-lifetime"
"dfs.namenode.max.objects"
"dfs.client.mmap.cache.timeout.ms"
"dfs.datatransfer.server.variablewhitelist.cache.secs"
"dfs.client.key.provider.cache.expiry"
"dfs.datatransfer.client.variablewhitelist.cache.secs"
"dfs.client.server-defaults.validity.period.ms"
"dfs.block.access.key.update.interval"
"dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold"
"dfs.block.access.token.lifetime"
"dfs.namenode.num.extra.edits.retained"
"dfs.client.hedged.read.threshold.millis"
"dfs.namenode.path.based.cache.retry.interval.ms"
"dfs.client.read.prefetch.size"
"dfs.namenode.checkpoint.txns"
"dfs.namenode.fs-limits.max-blocks-per-file"
"dfs.namenode.path.based.cache.refresh.interval.ms"
"dfs.client.read.short.circuit.replica.stale.threshold.ms"
"dfs.image.transfer.bandwidthPerSec"
"dfs.namenode.max-num-blocks-to-log"
"dfs.datanode.cached-dfsused.check.interval.ms"
"dfs.client.write.exclude.nodes.cache.expiry.interval.millis"
"dfs.namenode.resource.check.interval"
"dfs.namenode.blocks.per.postponedblocks.rescan"
"dfs.disk.balancer.block.tolerance.percent"
"dfs.journalnode.sync.interval"
"dfs.client.read.shortcircuit.streams.cache.expiry.ms"
"dfs.datanode.cache.revocation.polling.ms"
"dfs.edit.log.transfer.bandwidthPerSec"
"dfs.namenode.retrycache.expirytime.millis"
"dfs.datanode.cache.revocation.timeout.ms"
"dfs.client.mmap.retry.timeout.ms"
"dfs.namenode.accesstime.precision"
"dfs.domain.socket.disable.interval.seconds"
"dfs.namenode.write-lock-reporting-threshold-ms"
"dfs.namenode.storageinfo.defragment.timeout.ms"
"dfs.namenode.storageinfo.defragment.interval.ms"
"dfs.image.transfer-bootstrap-standby.bandwidthPerSec"
"dfs.datatransfer.client.variableBlackList.cache.secs"
"dfs.disk.balancer.max.disk.errors"
"dfs.namenode.fs-limits.min-block-size"
"dfs.namenode.delegation.token.renew-interval"
"dfs.namenode.read-lock-reporting-threshold-ms"
"dfs.namenode.lease-recheck-interval-ms"
"dfs.namenode.startup.delay.block.deletion.sec"
"dfs.namenode.stale.datanode.interval"
"dfs.client.slow.io.warning.threshold.ms"
"dfs.ha.tail-edits.max-txns-per-lock"
"dfs.client.write.byte-array-manager.count-reset-time-period-ms"
"dfs.mover.movedWinWidth"
"dfs.datatransfer.server.variableBlackList.cache.secs"
"dfs.namenode.max-lock-hold-to-release-lease-ms"
"dfs.content-summary.sleep-microsec"
"dfs.client.socketcache.expiryMsec"

119 parameters read from function getInt:
"dfs.mover.max-no-move-interval"
"dfs.provided.aliasmap.inmemory.batch-size"
"dfs.client.socketcache.capacity"
"dfs.client.max.block.acquire.failures"
"dfs.datanode.ec.reconstruction.stripedread.buffer.size"
"dfs.client.retry.times.get-last-block-length"
"dfs.namenode.edit.log.autoroll.check.interval.ms"
"dfs.namenode.inotify.max.events.per.rpc"
"dfs.client.write.byte-array-manager.count-limit"
"dfs.namenode.snapshot.skiplist.interval"
"dfs.namenode.file.close.num-committed-allowed"
"dfs.client.block.write.replace-datanode-on-failure.min-replication"
"dfs.client.read.striped.threadpool.size"
"dfs.namenode.max.extra.edits.segments.retained"
"dfs.namenode.list.cache.directives.num.responses"
"dfs.namenode.fs-limits.max-xattrs-per-inode"
"dfs.client.read.shortcircuit.buffer.size"
"dfs.namenode.lifeline.handler.count"
"dfs.namenode.replication.work.multiplier.per.iteration"
"dfs.datanode.directoryscan.throttle.limit.ms.per.sec"
"dfs.corruptfilesreturned.max"
"dfs.namenode.replication.max-streams"
"dfs.client.hedged.read.threadpool.size"
"dfs.namenode.list.cache.pools.num.responses"
"dfs.namenode.fs-limits.max-directory-items"
"dfs.namenode.decommission.max.concurrent.tracked.nodes"
"dfs.balancer.max-no-move-interval"
"dfs.datanode.socket.write.timeout"
"dfs.client.write.byte-array-manager.count-threshold"
"dfs.datanode.volumes.replica-add.threadpool.size"
"dfs.client.mmap.cache.size"
"dfs.namenode.replication.max-streams-hard-limit"
"dfs.namenode.list.openfiles.num.responses"
"dfs.namenode.maintenance.replication.min"
"dfs.namenode.edekcacheloader.initial.delay.ms"
"dfs.namenode.stale.datanode.minimum.interval"
"dfs.ha.tail-edits.namenode-retries"
"dfs.replication.max"
"dfs.datanode.lazywriter.interval.sec"
"dfs.client.block.write.locateFollowingBlock.retries"
"dfs.mover.moverThreads"
"dfs.namenode.max.full.block.report.leases"
"dfs.namenode.block.deletion.increment"
"dfs.namenode.snapshot.max.limit"
"dfs.client.cached.conn.retry"
"dfs.namenode.ec.policies.max.cellsize"
"dfs.ha.tail-edits.rolledits.timeout"
"dfs.datanode.parallel.volumes.load.threads.num"
"dfs.namenode.replication.min"
"dfs.encrypt.data.transfer.cipher.key.bitlength"
"dfs.replication"
"dfs.client.read.shortcircuit.metrics.sampling.percentage"
"dfs.namenode.safemode.replication.min"
"dfs.client.socket-timeout"
"dfs.client.failover.max.attempts"
"dfs.namenode.reencrypt.batch.size"
"dfs.namenode.snapshotdiff.listing.limit"
"dfs.datanode.fsdatasetcache.max.threads.per.volume"
"dfs.disk.balancer.max.disk.throughputInMBperSec"
"dfs.namenode.decommission.blocks.per.interval"
"dfs.namenode.top.num.users"
"dfs.namenode.upgrade.domain.factor"
"dfs.namenode.name.cache.threshold"
"dfs.client.retry.interval-ms.get-last-block-length"
"dfs.namenode.reconstruction.pending.timeout-sec"
"dfs.datanode.fileio.profiling.sampling.percentage"
"dfs.short.circuit.shared.memory.watcher.interrupt.check.ms"
"dfs.content-summary.limit"
"dfs.datanode.block.id.layout.upgrade.threads"
"dfs.namenode.reencrypt.edek.threads"
"dfs.namenode.max.op.size"
"dfs.client.retry.window.base"
"dfs.block.invalidate.limit"
"dfs.ha.zkfc.nn.http.timeout.ms"
"dfs.client-write-packet-size"
"dfs.client.block.write.retries"
"dfs.namenode.list.encryption.zones.num.responses"
"dfs.ha.zkfc.port"
"dfs.namenode.num.checkpoints.retained"
"dfs.datanode.balance.max.concurrent.moves"
"dfs.ls.limit"
"dfs.client.block.write.locateFollowingBlock.initial.delay.ms"
"dfs.edit.log.transfer.timeout"
"dfs.image.transfer.timeout"
"dfs.client.failover.sleep.base.millis"
"dfs.namenode.edekcacheloader.interval.ms"
"dfs.client.read.shortcircuit.streams.cache.size"
"dfs.datanode.ec.reconstruction.stripedread.timeout.millis"
"dfs.ha.log-roll.rpc.timeout"
"dfs.namenode.fs-limits.max-component-length"
"dfs.namenode.top.window.num.buckets"
"dfs.client.failover.sleep.max.millis"
"dfs.client.retry.max.attempts"
"dfs.namenode.list.reencryption.status.num.responses"
"dfs.namenode.service.handler.count"
"dfs.namenode.safemode.min.datanodes"
"dfs.datanode.directoryscan.threads"
"dfs.namenode.quota.init-threads"
"dfs.namenode.fs-limits.max-xattr-size"
"dfs.provided.aliasmap.load.retries"
"dfs.block.misreplication.processing.limit"
"dfs.namenode.heartbeat.recheck-interval"
"dfs.namenode.tolerate.heartbeat.multiplier"
"dfs.namenode.max-corrupt-file-blocks-returned"
"dfs.client.socket.send.buffer.size"
"dfs.namenode.edits.dir.minimum"
"dfs.namenode.metrics.logger.period.seconds"
"dfs.namenode.lazypersist.file.scrub.interval.sec"
"dfs.namenode.missing.checkpoint.periods.before.shutdown"
"dfs.namenode.snapshot.skiplist.max.levels"
"dfs.bytes-per-checksum"
"dfs.namenode.checkpoint.max-retries"
"dfs.image.transfer.chunksize"
"dfs.namenode.handler.count"
"dfs.mover.retry.max.attempts"
"dfs.balancer.block-move.timeout"
"dfs.namenode.resource.checked.volumes.minimum"
"dfs.client.write.max-packets-in-flight"
"dfs.client.test.drop.namenode.response.number"

1 parameters read from function getLongBytes:
"dfs.blocksize"

0 parameters read from function getHexDigits:

6 parameters read from function getDouble:
"dfs.namenode.checkpoint.check.quiet-multiplier"
"dfs.namenode.reencrypt.throttle.limit.updater.ratio"
"dfs.namenode.storageinfo.defragment.ratio"
"dfs.namenode.reencrypt.throttle.limit.handler.ratio"
"dfs.disk.balancer.plan.threshold.percent"
"dfs.namenode.redundancy.considerLoad.factor"

1 parameters read from function getInts:
"dfs.metrics.percentiles.intervals"

65 parameters read from function getBoolean:
"dfs.data.transfer.client.tcpnodelay"
"dfs.reformat.disabled"
"dfs.client.block.write.replace-datanode-on-failure.best-effort"
"dfs.permissions.enabled"
"dfs.balancer.keytab.enabled"
"dfs.namenode.lock.detailed-metrics.enabled"
"dfs.namenode.block-placement-policy.default.prefer-local-node"
"dfs.namenode.top.enabled"
"dfs.block.access.token.protobuf.enable"
"dfs.client.https.need-auth"
"dfs.client.mmap.enabled"
"dfs.namenode.redundancy.considerLoad"
"dfs.namenode.xattrs.enabled"
"dfs.use.dfs.network.topology"
"dfs.xframe.enabled"
"dfs.namenode.snapshot.capture.openfiles"
"dfs.client.read.shortcircuit.skip.checksum"
"dfs.namenode.delegation.token.always-use"
"dfs.image.string-tables.expanded"
"dfs.client.use.legacy.blockreader.local"
"dfs.namenode.name.dir.restore"
"dfs.namenode.avoid.read.stale.datanode"
"dfs.namenode.enable.retrycache"
"dfs.datatransfer.client.variableBlackList.enable"
"dfs.datatransfer.server.variableBlackList.enable"
"dfs.block.access.token.enable"
"dfs.journalnode.enable.sync"
"dfs.client.cache.drop.behind.reads"
"dfs.namenode.audit.log.token.tracking.id"
"dfs.namenode.audit.log.async"
"dfs.datanode.duplicate.replica.deletion"
"dfs.namenode.support.allow.format"
"dfs.datanode.peer.stats.enabled"
"dfs.datanode.block-pinning.enabled"
"dfs.namenode.datanode.registration.ip-hostname-check"
"dfs.provided.aliasmap.inmemory.enabled"
"dfs.namenode.reject-unresolved-dn-topology-mapping"
"dfs.client.block.write.replace-datanode-on-failure.enable"
"dfs.namenode.avoid.write.stale.datanode"
"dfs.namenode.snapshotdiff.allow.snap-root-descendant"
"dfs.storage.policy.enabled"
"dfs.ha.allow.stale.reads"
"dfs.namenode.posix.acl.inheritance.enabled"
"dfs.namenode.acls.enabled"
"dfs.namenode.snapshot.skip.capture.accesstime-only-change"
"dfs.client.domain.socket.data.traffic"
"dfs.namenode.edits.noeditlogchannelflush"
"dfs.quota.by.storage.type.enabled"
"dfs.client.cache.drop.behind.writes"
"dfs.client.use.datanode.hostname"
"dfs.encrypt.data.transfer"
"dfs.datatransfer.server.variablewhitelist.enable"
"dfs.disk.balancer.enabled"
"dfs.namenode.provided.enabled"
"dfs.webhdfs.rest-csrf.enabled"
"dfs.image.compress"
"dfs.datanode.enable.fileio.fault.injection"
"dfs.datatransfer.client.variablewhitelist.enable"
"dfs.mover.keytab.enabled"
"dfs.client.write.byte-array-manager.enabled"
"dfs.namenode.edits.asynclogging"
"dfs.ha.automatic-failover.enabled"
"dfs.ha.standby.checkpoints"
"dfs.namenode.fslock.fair"
"dfs.ha.tail-edits.in-progress"

25 parameters read from function get1:
"dfs.datanode.data.dir"
"dfs.namenode.top.window.num.buckets"
"dfs.namenode.top.num.users"
"dfs.web.authentication.kerberos.keytab"
"dfs.nameservice.id"
"dfs.provided.aliasmap.text.codec"
"dfs.client.cache.readahead"
"dfs.encrypt.data.transfer.cipher.suites"
"dfs.namenode.legacy-oiv-image.dir"
"dfs.web.authentication.simple.anonymous.allowed"
"dfs.namenode.top.windows.minutes"
"dfs.namenode.decommission.nodes.per.interval"
"dfs.metrics.percentiles.intervals"
"dfs.nameservices"
"dfs.namenode.shared.edits.dir"
"dfs.namenode.plugins"
"dfs.client.cache.drop.behind.writes"
"dfs.provided.aliasmap.leveldb.path"
"dfs.encrypt.data.transfer.algorithm"
"dfs.ha.namenode.id"
"dfs.client.cache.drop.behind.reads"
"dfs.data.transfer.protection"
"dfs.metrics.session-id"
"dfs.web.authentication.kerberos.principal"
"dfs.namenode.rpc-address"

10 parameters read from function getFloat:
"dfs.namenode.write.stale.datanode.ratio"
"dfs.namenode.safemode.threshold-pct"
"dfs.namenode.invalidate.work.pct.per.iteration"
"dfs.namenode.retrycache.heap.percent"
"dfs.namenode.replqueue.threshold-pct"
"dfs.namenode.path.based.cache.block.map.allocation.percent"
"dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction"
"dfs.namenode.lifeline.handler.ratio"
"dfs.namenode.edit.log.autoroll.multiplier.threshold"
"dfs.namenode.available-space-block-placement-policy.balanced-space-preference-fraction"

42 parameters read from function get2:
"dfs.client.context"
"dfs.https.server.keystore.resource"
"dfs.cluster.administrators"
"dfs.datatransfer.client.variableBlackList.file"
"dfs.disk.balancer.plan.valid.interval"
"dfs.datatransfer.client.variablewhitelist.file"
"dfs.namenode.min.supported.datanode.version"
"dfs.datatransfer.client.fixedBlackList.file"
"dfs.web.authentication.filter"
"dfs.mover.address"
"dfs.client.block.write.replace-datanode-on-failure.policy"
"dfs.hosts"
"dfs.provided.aliasmap.text.delimiter"
"dfs.user.home.dir.prefix"
"dfs.datanode.startup"
"dfs.image.compression.codec"
"dfs.datatransfer.client.fixedwhitelist.file"
"dfs.webhdfs.acl.provider.permission.pattern"
"dfs.balancer.address"
"dfs.provided.aliasmap.inmemory.dnrpc-address"
"dfs.datatransfer.server.variablewhitelist.file"
"dfs.provided.aliasmap.text.read.file"
"dfs.checksum.combine.mode"
"dfs.permissions.superusergroup"
"dfs.datanode.kerberos.principal"
"dfs.journalnode.edits.dir"
"dfs.journalnode.https-address"
"dfs.datatransfer.server.fixedwhitelist.file"
"dfs.journalnode.http-address"
"dfs.http.policy"
"dfs.datatransfer.server.variableBlackList.file"
"dfs.hosts.exclude"
"dfs.datatransfer.server.fixedBlackList.file"
"dfs.namenode.kerberos.principal"
"dfs.provided.storage.id"
"dfs.webhdfs.user.provider.user.pattern"
"dfs.journalnode.rpc-address"
"dfs.provided.aliasmap.text.write.dir"
"dfs.checksum.type"
"dfs.datanode.data.dir.perm"
"dfs.namenode.startup"
"dfs.namenode.backup.http-address"


org.apache.hadoop.hdfs.server.datanode.DataNode
edgeAnalysis for function get [java.lang.String]
edgeAnalysis for function get [java.lang.String, java.lang.String]
edgeAnalysis for function getInt
edgeAnalysis for function getInts
edgeAnalysis for function getLong
edgeAnalysis for function getLongBytes
edgeAnalysis for function getHexDigits
edgeAnalysis for function getFloat
edgeAnalysis for function getDouble
edgeAnalysis for function getBoolean
Results: parameters statically used on component org.apache.hadoop.hdfs.server.datanode.DataNode
46 parameters read from function getLong:
"dfs.disk.balancer.max.disk.throughputInMBperSec"
"dfs.image.transfer.bandwidthPerSec"
"dfs.namenode.max-num-blocks-to-log"
"dfs.datanode.cached-dfsused.check.interval.ms"
"dfs.client.write.exclude.nodes.cache.expiry.interval.millis"
"dfs.client.cache.readahead"
"dfs.datanode.restart.replica.expiration"
"dfs.datanode.max.locked.memory"
"dfs.datanode.readahead.bytes"
"dfs.datanode.lifeline.interval.seconds"
"dfs.blockreport.intervalMsec"
"dfs.client.mmap.cache.timeout.ms"
"dfs.datanode.scan.period.hours"
"dfs.datatransfer.server.variablewhitelist.cache.secs"
"dfs.disk.balancer.block.tolerance.percent"
"dfs.journalnode.sync.interval"
"dfs.client.read.shortcircuit.streams.cache.expiry.ms"
"dfs.datanode.cache.revocation.polling.ms"
"dfs.client.key.provider.cache.expiry"
"dfs.blockreport.incremental.intervalMsec"
"dfs.blockreport.split.threshold"
"dfs.edit.log.transfer.bandwidthPerSec"
"dfs.datanode.cache.revocation.timeout.ms"
"dfs.client.mmap.retry.timeout.ms"
"dfs.domain.socket.disable.interval.seconds"
"dfs.image.transfer-bootstrap-standby.bandwidthPerSec"
"dfs.datanode.slow.io.warning.threshold.ms"
"dfs.datatransfer.client.variableBlackList.cache.secs"
"dfs.datatransfer.client.variablewhitelist.cache.secs"
"dfs.disk.balancer.max.disk.errors"
"dfs.client.server-defaults.validity.period.ms"
"dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold"
"dfs.namenode.num.extra.edits.retained"
"dfs.cachereport.intervalMsec"
"dfs.client.hedged.read.threshold.millis"
"dfs.namenode.stale.datanode.interval"
"dfs.client.slow.io.warning.threshold.ms"
"dfs.client.read.prefetch.size"
"dfs.datanode.xceiver.stop.timeout.millis"
"dfs.namenode.checkpoint.txns"
"dfs.ha.tail-edits.max-txns-per-lock"
"dfs.client.write.byte-array-manager.count-reset-time-period-ms"
"dfs.datatransfer.server.variableBlackList.cache.secs"
"dfs.block.scanner.volume.bytes.per.second"
"dfs.client.read.short.circuit.replica.stale.threshold.ms"
"dfs.client.socketcache.expiryMsec"

75 parameters read from function getInt:
"dfs.datanode.fsdatasetcache.max.threads.per.volume"
"dfs.disk.balancer.max.disk.throughputInMBperSec"
"dfs.datanode.max.transfer.threads"
"dfs.provided.aliasmap.inmemory.batch-size"
"dfs.namenode.upgrade.domain.factor"
"dfs.client.socketcache.capacity"
"dfs.client.retry.interval-ms.get-last-block-length"
"dfs.datanode.fileio.profiling.sampling.percentage"
"dfs.short.circuit.shared.memory.watcher.interrupt.check.ms"
"dfs.datanode.block.id.layout.upgrade.threads"
"dfs.client.max.block.acquire.failures"
"dfs.datanode.ec.reconstruction.stripedread.buffer.size"
"dfs.client.retry.times.get-last-block-length"
"dfs.client.retry.window.base"
"dfs.namenode.inotify.max.events.per.rpc"
"dfs.client.write.byte-array-manager.count-limit"
"dfs.client.read.striped.threadpool.size"
"dfs.ha.zkfc.nn.http.timeout.ms"
"dfs.client.block.write.replace-datanode-on-failure.min-replication"
"dfs.namenode.max.extra.edits.segments.retained"
"dfs.client-write-packet-size"
"dfs.datanode.metrics.logger.period.seconds"
"dfs.client.block.write.retries"
"dfs.client.read.shortcircuit.buffer.size"
"dfs.datanode.transfer.socket.recv.buffer.size"
"dfs.namenode.num.checkpoints.retained"
"dfs.ha.zkfc.port"
"dfs.webhdfs.netty.high.watermark"
"dfs.datanode.balance.max.concurrent.moves"
"dfs.client.block.write.locateFollowingBlock.initial.delay.ms"
"dfs.edit.log.transfer.timeout"
"dfs.image.transfer.timeout"
"dfs.client.failover.sleep.base.millis"
"dfs.datanode.directoryscan.throttle.limit.ms.per.sec"
"dfs.client.hedged.read.threadpool.size"
"dfs.datanode.socket.write.timeout"
"dfs.datanode.transfer.socket.send.buffer.size"
"dfs.client.read.shortcircuit.streams.cache.size"
"dfs.client.write.byte-array-manager.count-threshold"
"dfs.datanode.ec.reconstruction.stripedread.timeout.millis"
"dfs.datanode.volumes.replica-add.threadpool.size"
"dfs.ha.log-roll.rpc.timeout"
"dfs.client.mmap.cache.size"
"dfs.datanode.handler.count"
"dfs.datanode.ec.reconstruction.threads"
"dfs.client.failover.sleep.max.millis"
"dfs.ha.tail-edits.namenode-retries"
"dfs.client.retry.max.attempts"
"dfs.datanode.lazywriter.interval.sec"
"dfs.client.block.write.locateFollowingBlock.retries"
"dfs.webhdfs.ugi.expire.after.access"
"dfs.datanode.http.internal-proxy.port"
"dfs.client.cached.conn.retry"
"dfs.datanode.directoryscan.threads"
"dfs.provided.aliasmap.load.retries"
"dfs.namenode.tolerate.heartbeat.multiplier"
"dfs.ha.tail-edits.rolledits.timeout"
"dfs.client.socket.send.buffer.size"
"dfs.namenode.edits.dir.minimum"
"dfs.datanode.parallel.volumes.load.threads.num"
"dfs.datanode.network.counts.cache.max.size"
"dfs.namenode.replication.min"
"dfs.datanode.socket.reuse.keepalive"
"dfs.encrypt.data.transfer.cipher.key.bitlength"
"dfs.webhdfs.netty.low.watermark"
"dfs.replication"
"dfs.client.read.shortcircuit.metrics.sampling.percentage"
"dfs.bytes-per-checksum"
"dfs.datanode.failed.volumes.tolerated"
"dfs.namenode.checkpoint.max-retries"
"dfs.image.transfer.chunksize"
"dfs.client.socket-timeout"
"dfs.client.write.max-packets-in-flight"
"dfs.client.failover.max.attempts"
"dfs.client.test.drop.namenode.response.number"

2 parameters read from function getLongBytes:
"dfs.blocksize"
"dfs.datanode.balance.bandwidthPerSec"

0 parameters read from function getHexDigits:

2 parameters read from function getDouble:
"dfs.namenode.checkpoint.check.quiet-multiplier"
"dfs.namenode.redundancy.considerLoad.factor"

1 parameters read from function getInts:
"dfs.metrics.percentiles.intervals"

45 parameters read from function getBoolean:
"dfs.datanode.block-pinning.enabled"
"dfs.datanode.synconclose"
"dfs.data.transfer.client.tcpnodelay"
"dfs.client.block.write.replace-datanode-on-failure.enable"
"dfs.datanode.transferTo.allowed"
"dfs.client.block.write.replace-datanode-on-failure.best-effort"
"dfs.permissions.enabled"
"dfs.pipeline.ecn"
"dfs.datanode.drop.cache.behind.writes"
"dfs.namenode.block-placement-policy.default.prefer-local-node"
"dfs.datanode.drop.cache.behind.reads"
"dfs.block.access.token.protobuf.enable"
"dfs.client.domain.socket.data.traffic"
"dfs.namenode.redundancy.considerLoad"
"dfs.namenode.edits.noeditlogchannelflush"
"dfs.client.mmap.enabled"
"dfs.datanode.non.local.lazy.persist"
"dfs.client.use.datanode.hostname"
"dfs.client.cache.drop.behind.writes"
"dfs.xframe.enabled"
"dfs.encrypt.data.transfer"
"dfs.client.read.shortcircuit.skip.checksum"
"dfs.datatransfer.server.variablewhitelist.enable"
"dfs.disk.balancer.enabled"
"dfs.data.transfer.server.tcpnodelay"
"dfs.client.use.legacy.blockreader.local"
"dfs.namenode.name.dir.restore"
"dfs.datanode.use.datanode.hostname"
"dfs.webhdfs.rest-csrf.enabled"
"dfs.image.compress"
"dfs.datatransfer.client.variableBlackList.enable"
"dfs.datanode.enable.fileio.fault.injection"
"dfs.datatransfer.client.variablewhitelist.enable"
"dfs.client.write.byte-array-manager.enabled"
"dfs.namenode.edits.asynclogging"
"dfs.datatransfer.server.variableBlackList.enable"
"dfs.block.access.token.enable"
"dfs.ha.automatic-failover.enabled"
"dfs.journalnode.enable.sync"
"dfs.datanode.sync.behind.writes.in.background"
"dfs.client.cache.drop.behind.reads"
"dfs.datanode.duplicate.replica.deletion"
"dfs.datanode.sync.behind.writes"
"dfs.datanode.peer.stats.enabled"
"dfs.ha.tail-edits.in-progress"

20 parameters read from function get1:
"dfs.namenode.shared.edits.dir"
"dfs.datanode.data.dir"
"dfs.client.cache.drop.behind.writes"
"dfs.nameservice.id"
"dfs.provided.aliasmap.text.codec"
"dfs.client.cache.readahead"
"dfs.encrypt.data.transfer.cipher.suites"
"dfs.namenode.legacy-oiv-image.dir"
"dfs.datanode.plugins"
"dfs.datanode.hostname"
"dfs.provided.aliasmap.leveldb.path"
"dfs.encrypt.data.transfer.algorithm"
"dfs.datanode.dns.interface"
"dfs.ha.namenode.id"
"dfs.client.cache.drop.behind.reads"
"dfs.data.transfer.protection"
"dfs.metrics.session-id"
"dfs.namenode.rpc-address"
"dfs.nameservices"
"dfs.datanode.dns.nameserver"

3 parameters read from function getFloat:
"dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction"
"dfs.datanode.ec.reconstruction.xmits.weight"
"dfs.namenode.available-space-block-placement-policy.balanced-space-preference-fraction"

32 parameters read from function get2:
"dfs.client.context"
"dfs.cluster.administrators"
"dfs.datatransfer.client.variableBlackList.file"
"dfs.disk.balancer.plan.valid.interval"
"dfs.datatransfer.client.variablewhitelist.file"
"dfs.datatransfer.client.fixedBlackList.file"
"dfs.client.block.write.replace-datanode-on-failure.policy"
"dfs.hosts"
"dfs.provided.aliasmap.text.delimiter"
"dfs.user.home.dir.prefix"
"dfs.datanode.startup"
"dfs.image.compression.codec"
"dfs.datatransfer.client.fixedwhitelist.file"
"dfs.datatransfer.server.variablewhitelist.file"
"dfs.datanode.oob.timeout-ms"
"dfs.provided.aliasmap.text.read.file"
"hadoop.hdfs.configuration.version"
"dfs.checksum.combine.mode"
"dfs.permissions.superusergroup"
"dfs.journalnode.edits.dir"
"dfs.datatransfer.server.fixedwhitelist.file"
"dfs.http.policy"
"dfs.datatransfer.server.variableBlackList.file"
"dfs.datanode.min.supported.namenode.version"
"dfs.hosts.exclude"
"dfs.datatransfer.server.fixedBlackList.file"
"dfs.namenode.kerberos.principal"
"dfs.provided.storage.id"
"dfs.provided.aliasmap.text.write.dir"
"dfs.checksum.type"
"dfs.datanode.data.dir.perm"
"dfs.namenode.startup"


org.apache.hadoop.hdfs.qjournal.server.JournalNode
edgeAnalysis for function get [java.lang.String]
edgeAnalysis for function get [java.lang.String, java.lang.String]
edgeAnalysis for function getInt
edgeAnalysis for function getInts
edgeAnalysis for function getLong
edgeAnalysis for function getLongBytes
edgeAnalysis for function getHexDigits
edgeAnalysis for function getFloat
edgeAnalysis for function getDouble
edgeAnalysis for function getBoolean
Results: parameters statically used on component org.apache.hadoop.hdfs.qjournal.server.JournalNode
35 parameters read from function getLong:
"dfs.disk.balancer.max.disk.throughputInMBperSec"
"dfs.image.transfer.bandwidthPerSec"
"dfs.balancer.max-iteration-time"
"dfs.datanode.cached-dfsused.check.interval.ms"
"dfs.client.cache.readahead"
"dfs.client.write.exclude.nodes.cache.expiry.interval.millis"
"dfs.client.mmap.cache.timeout.ms"
"dfs.datatransfer.server.variablewhitelist.cache.secs"
"dfs.disk.balancer.block.tolerance.percent"
"dfs.journalnode.sync.interval"
"dfs.client.read.shortcircuit.streams.cache.expiry.ms"
"dfs.datanode.cache.revocation.polling.ms"
"dfs.client.key.provider.cache.expiry"
"dfs.edit.log.transfer.bandwidthPerSec"
"dfs.datanode.cache.revocation.timeout.ms"
"dfs.client.mmap.retry.timeout.ms"
"dfs.domain.socket.disable.interval.seconds"
"dfs.image.transfer-bootstrap-standby.bandwidthPerSec"
"dfs.datatransfer.client.variableBlackList.cache.secs"
"dfs.datatransfer.client.variablewhitelist.cache.secs"
"dfs.disk.balancer.max.disk.errors"
"dfs.client.server-defaults.validity.period.ms"
"dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold"
"dfs.namenode.num.extra.edits.retained"
"dfs.client.hedged.read.threshold.millis"
"dfs.namenode.stale.datanode.interval"
"dfs.client.slow.io.warning.threshold.ms"
"dfs.client.read.prefetch.size"
"dfs.namenode.checkpoint.txns"
"dfs.ha.tail-edits.max-txns-per-lock"
"dfs.client.write.byte-array-manager.count-reset-time-period-ms"
"dfs.mover.movedWinWidth"
"dfs.datatransfer.server.variableBlackList.cache.secs"
"dfs.client.read.short.circuit.replica.stale.threshold.ms"
"dfs.client.socketcache.expiryMsec"

68 parameters read from function getInt:
"dfs.datanode.fsdatasetcache.max.threads.per.volume"
"dfs.disk.balancer.max.disk.throughputInMBperSec"
"dfs.mover.max-no-move-interval"
"dfs.provided.aliasmap.inmemory.batch-size"
"dfs.namenode.upgrade.domain.factor"
"dfs.client.socketcache.capacity"
"dfs.client.retry.interval-ms.get-last-block-length"
"dfs.datanode.fileio.profiling.sampling.percentage"
"dfs.short.circuit.shared.memory.watcher.interrupt.check.ms"
"dfs.datanode.block.id.layout.upgrade.threads"
"dfs.client.max.block.acquire.failures"
"dfs.datanode.ec.reconstruction.stripedread.buffer.size"
"dfs.client.retry.times.get-last-block-length"
"dfs.client.retry.window.base"
"dfs.namenode.inotify.max.events.per.rpc"
"dfs.client.write.byte-array-manager.count-limit"
"dfs.ha.zkfc.nn.http.timeout.ms"
"dfs.client.block.write.replace-datanode-on-failure.min-replication"
"dfs.client.read.striped.threadpool.size"
"dfs.namenode.max.extra.edits.segments.retained"
"dfs.client-write-packet-size"
"dfs.client.block.write.retries"
"dfs.client.read.shortcircuit.buffer.size"
"dfs.namenode.num.checkpoints.retained"
"dfs.ha.zkfc.port"
"dfs.datanode.balance.max.concurrent.moves"
"dfs.client.block.write.locateFollowingBlock.initial.delay.ms"
"dfs.edit.log.transfer.timeout"
"dfs.image.transfer.timeout"
"dfs.client.failover.sleep.base.millis"
"dfs.datanode.directoryscan.throttle.limit.ms.per.sec"
"dfs.client.hedged.read.threadpool.size"
"dfs.balancer.max-no-move-interval"
"dfs.datanode.socket.write.timeout"
"dfs.client.read.shortcircuit.streams.cache.size"
"dfs.client.write.byte-array-manager.count-threshold"
"dfs.datanode.ec.reconstruction.stripedread.timeout.millis"
"dfs.datanode.volumes.replica-add.threadpool.size"
"dfs.ha.log-roll.rpc.timeout"
"dfs.client.mmap.cache.size"
"dfs.client.failover.sleep.max.millis"
"dfs.ha.tail-edits.namenode-retries"
"dfs.client.retry.max.attempts"
"dfs.datanode.lazywriter.interval.sec"
"dfs.client.block.write.locateFollowingBlock.retries"
"dfs.mover.moverThreads"
"dfs.client.cached.conn.retry"
"dfs.datanode.directoryscan.threads"
"dfs.provided.aliasmap.load.retries"
"dfs.namenode.tolerate.heartbeat.multiplier"
"dfs.ha.tail-edits.rolledits.timeout"
"dfs.client.socket.send.buffer.size"
"dfs.namenode.edits.dir.minimum"
"dfs.datanode.parallel.volumes.load.threads.num"
"dfs.namenode.replication.min"
"dfs.encrypt.data.transfer.cipher.key.bitlength"
"dfs.replication"
"dfs.client.read.shortcircuit.metrics.sampling.percentage"
"dfs.namenode.missing.checkpoint.periods.before.shutdown"
"dfs.bytes-per-checksum"
"dfs.namenode.checkpoint.max-retries"
"dfs.image.transfer.chunksize"
"dfs.client.socket-timeout"
"dfs.mover.retry.max.attempts"
"dfs.balancer.block-move.timeout"
"dfs.client.write.max-packets-in-flight"
"dfs.client.failover.max.attempts"
"dfs.client.test.drop.namenode.response.number"

1 parameters read from function getLongBytes:
"dfs.blocksize"

0 parameters read from function getHexDigits:

3 parameters read from function getDouble:
"dfs.namenode.checkpoint.check.quiet-multiplier"
"dfs.disk.balancer.plan.threshold.percent"
"dfs.namenode.redundancy.considerLoad.factor"

1 parameters read from function getInts:
"dfs.metrics.percentiles.intervals"

33 parameters read from function getBoolean:
"dfs.datanode.block-pinning.enabled"
"dfs.data.transfer.client.tcpnodelay"
"dfs.client.block.write.replace-datanode-on-failure.enable"
"dfs.client.block.write.replace-datanode-on-failure.best-effort"
"dfs.balancer.keytab.enabled"
"dfs.namenode.block-placement-policy.default.prefer-local-node"
"dfs.block.access.token.protobuf.enable"
"dfs.client.domain.socket.data.traffic"
"dfs.client.https.need-auth"
"dfs.client.mmap.enabled"
"dfs.namenode.edits.noeditlogchannelflush"
"dfs.namenode.redundancy.considerLoad"
"dfs.client.cache.drop.behind.writes"
"dfs.client.use.datanode.hostname"
"dfs.client.read.shortcircuit.skip.checksum"
"dfs.datatransfer.server.variablewhitelist.enable"
"dfs.disk.balancer.enabled"
"dfs.client.use.legacy.blockreader.local"
"dfs.namenode.name.dir.restore"
"dfs.image.compress"
"dfs.datatransfer.client.variableBlackList.enable"
"dfs.datanode.enable.fileio.fault.injection"
"dfs.datatransfer.client.variablewhitelist.enable"
"dfs.mover.keytab.enabled"
"dfs.client.write.byte-array-manager.enabled"
"dfs.namenode.edits.asynclogging"
"dfs.datatransfer.server.variableBlackList.enable"
"dfs.journalnode.enable.sync"
"dfs.ha.automatic-failover.enabled"
"dfs.client.cache.drop.behind.reads"
"dfs.datanode.duplicate.replica.deletion"
"dfs.namenode.support.allow.format"
"dfs.ha.tail-edits.in-progress"

17 parameters read from function get1:
"dfs.namenode.shared.edits.dir"
"dfs.datanode.data.dir"
"dfs.client.cache.drop.behind.writes"
"dfs.nameservice.id"
"dfs.web.authentication.kerberos.keytab"
"dfs.provided.aliasmap.text.codec"
"dfs.client.cache.readahead"
"dfs.encrypt.data.transfer.cipher.suites"
"dfs.namenode.legacy-oiv-image.dir"
"dfs.provided.aliasmap.leveldb.path"
"dfs.encrypt.data.transfer.algorithm"
"dfs.ha.namenode.id"
"dfs.client.cache.drop.behind.reads"
"dfs.data.transfer.protection"
"dfs.metrics.session-id"
"dfs.namenode.rpc-address"
"dfs.nameservices"

2 parameters read from function getFloat:
"dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction"
"dfs.namenode.available-space-block-placement-policy.balanced-space-preference-fraction"

35 parameters read from function get2:
"dfs.client.context"
"dfs.https.server.keystore.resource"
"dfs.cluster.administrators"
"dfs.datatransfer.client.variableBlackList.file"
"dfs.disk.balancer.plan.valid.interval"
"dfs.datatransfer.client.variablewhitelist.file"
"dfs.datatransfer.client.fixedBlackList.file"
"dfs.mover.address"
"dfs.client.block.write.replace-datanode-on-failure.policy"
"dfs.hosts"
"dfs.provided.aliasmap.text.delimiter"
"dfs.user.home.dir.prefix"
"dfs.datanode.startup"
"dfs.image.compression.codec"
"dfs.datatransfer.client.fixedwhitelist.file"
"dfs.balancer.address"
"dfs.datatransfer.server.variablewhitelist.file"
"dfs.provided.aliasmap.text.read.file"
"dfs.checksum.combine.mode"
"dfs.datanode.kerberos.principal"
"dfs.journalnode.edits.dir"
"dfs.journalnode.https-address"
"dfs.datatransfer.server.fixedwhitelist.file"
"dfs.journalnode.http-address"
"dfs.http.policy"
"dfs.datatransfer.server.variableBlackList.file"
"dfs.hosts.exclude"
"dfs.datatransfer.server.fixedBlackList.file"
"dfs.namenode.kerberos.principal"
"dfs.provided.storage.id"
"dfs.provided.aliasmap.text.write.dir"
"dfs.journalnode.rpc-address"
"dfs.checksum.type"
"dfs.datanode.data.dir.perm"
"dfs.namenode.startup"


