edgeAnalysis for function get [java.lang.String]
edgeAnalysis for function get [java.lang.String, java.lang.String]
edgeAnalysis for function getInt
edgeAnalysis for function getInts
edgeAnalysis for function getLong
edgeAnalysis for function getLongBytes
edgeAnalysis for function getHexDigits
edgeAnalysis for function getFloat
edgeAnalysis for function getDouble
edgeAnalysis for function getBoolean
Results: parameters statically used on component org.apache.hadoop.hdfs.qjournal.server.JournalNode
35 parameters read from function getLong:
"dfs.disk.balancer.max.disk.throughputInMBperSec" 10L
"dfs.client.hedged.read.threshold.millis" 500L
"dfs.datanode.cached-dfsused.check.interval.ms" 600000L
"dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold" 10737418240L
"dfs.journalnode.sync.interval" 120000L
"dfs.datatransfer.client.variableBlackList.cache.secs" 3600L
"dfs.client.mmap.cache.timeout.ms" 3600000L
"dfs.namenode.num.extra.edits.retained" 1000000L
"dfs.datanode.cache.revocation.polling.ms" 500L
"dfs.client.socketcache.expiryMsec" 3000L
"dfs.client.read.shortcircuit.streams.cache.expiry.ms" 300000L
"dfs.client.write.byte-array-manager.count-reset-time-period-ms" 10000L
"dfs.namenode.stale.datanode.interval" 30000L
"dfs.datatransfer.server.variableBlackList.cache.secs" 3600L
"dfs.datatransfer.server.variablewhitelist.cache.secs" 3600L
"dfs.ha.tail-edits.max-txns-per-lock" 9223372036854775807L
"dfs.client.mmap.retry.timeout.ms" 300000L
"dfs.client.slow.io.warning.threshold.ms" 30000L
"dfs.client.write.exclude.nodes.cache.expiry.interval.millis" 600000L
"dfs.datatransfer.client.variablewhitelist.cache.secs" 3600L
"dfs.edit.log.transfer.bandwidthPerSec" 0L
"dfs.image.transfer-bootstrap-standby.bandwidthPerSec" 0L
"dfs.namenode.checkpoint.txns" 1000000L
"dfs.client.server-defaults.validity.period.ms" $l7
"dfs.image.transfer.bandwidthPerSec" 0L
"dfs.disk.balancer.block.tolerance.percent" 10L
"dfs.client.read.short.circuit.replica.stale.threshold.ms" 1800000L
"dfs.client.read.prefetch.size" $l23
"dfs.balancer.max-iteration-time" 1200000L
"dfs.mover.movedWinWidth" 5400000L
"dfs.domain.socket.disable.interval.seconds" 600L
"dfs.client.key.provider.cache.expiry" $l3
"dfs.datanode.cache.revocation.timeout.ms" 900000L
"dfs.disk.balancer.max.disk.errors" 5L
"dfs.client.cache.readahead" 0L

68 parameters read from function getInt:
"dfs.client.write.byte-array-manager.count-threshold" 128
"dfs.image.transfer.timeout" 60000
"dfs.client.block.write.locateFollowingBlock.initial.delay.ms" 400
"dfs.namenode.checkpoint.max-retries" 3
"dfs.provided.aliasmap.inmemory.batch-size" 500
"dfs.provided.aliasmap.load.retries" 0
"dfs.namenode.num.checkpoints.retained" 2
"dfs.datanode.directoryscan.threads" 1
"dfs.mover.max-no-move-interval" 60000
"dfs.datanode.block.id.layout.upgrade.threads" 12
"dfs.namenode.upgrade.domain.factor" 3
"dfs.ha.zkfc.port" 8019
"dfs.namenode.inotify.max.events.per.rpc" 1000
"dfs.datanode.ec.reconstruction.stripedread.buffer.size" 65536
"dfs.balancer.block-move.timeout" 0
"dfs.client.failover.max.attempts" 15
"dfs.datanode.ec.reconstruction.stripedread.timeout.millis" 5000
"dfs.client.cached.conn.retry" 3
"dfs.datanode.socket.write.timeout" 480000
"dfs.namenode.edits.dir.minimum" 1
"dfs.datanode.parallel.volumes.load.threads.num" i0
"dfs.client.failover.sleep.base.millis" 500
"dfs.client.read.striped.threadpool.size" 18
"dfs.client.socketcache.capacity" 16
"dfs.client.retry.interval-ms.get-last-block-length" 4000
"dfs.ha.zkfc.nn.http.timeout.ms" 20000
"dfs.client.retry.max.attempts" 10
"dfs.client.socket-timeout" 60000
"dfs.client.block.write.replace-datanode-on-failure.min-replication" 0
"dfs.namenode.replication.min" 1
"dfs.ha.tail-edits.namenode-retries" 3
"dfs.client.block.write.locateFollowingBlock.retries" 5
"dfs.client.retry.window.base" 3000
"dfs.edit.log.transfer.timeout" 30000
"dfs.datanode.fsdatasetcache.max.threads.per.volume" 4
"dfs.namenode.tolerate.heartbeat.multiplier" 4
"dfs.client.retry.times.get-last-block-length" 3
"dfs.balancer.max-no-move-interval" 60000
"dfs.client.read.shortcircuit.streams.cache.size" 256
"dfs.client.block.write.retries" 3
"dfs.ha.log-roll.rpc.timeout" 20000
"dfs.image.transfer.chunksize" 65536
"dfs.client.hedged.read.threadpool.size" 0
"dfs.datanode.volumes.replica-add.threadpool.size" i1
"dfs.short.circuit.shared.memory.watcher.interrupt.check.ms" 60000
"dfs.disk.balancer.max.disk.throughputInMBperSec" 10
"dfs.client-write-packet-size" 65536
"dfs.client.max.block.acquire.failures" 3
"dfs.datanode.directoryscan.throttle.limit.ms.per.sec" 1000
"dfs.mover.retry.max.attempts" 10
"dfs.client.write.max-packets-in-flight" 80
"dfs.client.socket.send.buffer.size" 0
"dfs.namenode.max.extra.edits.segments.retained" 10000
"dfs.datanode.fileio.profiling.sampling.percentage" 0
"dfs.encrypt.data.transfer.cipher.key.bitlength" 128
"dfs.mover.moverThreads" 1000
"dfs.ha.tail-edits.rolledits.timeout" 60
"dfs.client.failover.sleep.max.millis" 15000
"dfs.namenode.missing.checkpoint.periods.before.shutdown" 3
"dfs.datanode.balance.max.concurrent.moves" 50
"dfs.client.read.shortcircuit.buffer.size" 1048576
"dfs.client.test.drop.namenode.response.number" 0
"dfs.replication" 3
"dfs.client.read.shortcircuit.metrics.sampling.percentage" 0
"dfs.client.write.byte-array-manager.count-limit" 2048
"dfs.bytes-per-checksum" 512
"dfs.datanode.lazywriter.interval.sec" 60
"dfs.client.mmap.cache.size" 256

1 parameters read from function getLongBytes:
"dfs.blocksize" 134217728L

0 parameters read from function getHexDigits:

3 parameters read from function getDouble:
"dfs.namenode.redundancy.considerLoad.factor" 2.0
"dfs.namenode.checkpoint.check.quiet-multiplier" 1.5
"dfs.disk.balancer.plan.threshold.percent" 10.0

1 parameters read from function getInts:
"dfs.metrics.percentiles.intervals"

33 parameters read from function getBoolean:
"dfs.image.compress" 0
"dfs.data.transfer.client.tcpnodelay" 1
"dfs.namenode.redundancy.considerLoad" 1
"dfs.client.mmap.enabled" 1
"dfs.datatransfer.server.variableBlackList.enable" 0
"dfs.datanode.duplicate.replica.deletion" 1
"dfs.client.cache.drop.behind.reads" 0
"dfs.client.https.need-auth" 0
"dfs.namenode.name.dir.restore" 0
"dfs.datanode.block-pinning.enabled" 0
"dfs.client.write.byte-array-manager.enabled" 0
"dfs.client.use.datanode.hostname" 0
"dfs.client.block.write.replace-datanode-on-failure.enable" 1
"dfs.client.read.shortcircuit.skip.checksum" 0
"dfs.datatransfer.server.variablewhitelist.enable" 0
"dfs.datatransfer.client.variableBlackList.enable" 0
"dfs.balancer.keytab.enabled" 0
"dfs.namenode.support.allow.format" 1
"dfs.namenode.block-placement-policy.default.prefer-local-node" 1
"dfs.ha.tail-edits.in-progress" 0
"dfs.datatransfer.client.variablewhitelist.enable" 0
"dfs.client.cache.drop.behind.writes" 0
"dfs.datanode.enable.fileio.fault.injection" 0
"dfs.ha.automatic-failover.enabled" 0
"dfs.client.domain.socket.data.traffic" 0
"dfs.namenode.edits.noeditlogchannelflush" 0
"dfs.client.use.legacy.blockreader.local" 0
"dfs.client.block.write.replace-datanode-on-failure.best-effort" 0
"dfs.block.access.token.protobuf.enable" 0
"dfs.namenode.edits.asynclogging" 1
"dfs.mover.keytab.enabled" 0
"dfs.journalnode.enable.sync" 1
"dfs.disk.balancer.enabled" 1

17 parameters read from function get1:
"dfs.namenode.shared.edits.dir"
"dfs.datanode.data.dir"
"dfs.client.cache.drop.behind.writes"
"dfs.nameservice.id"
"dfs.web.authentication.kerberos.keytab"
"dfs.provided.aliasmap.text.codec"
"dfs.client.cache.readahead"
"dfs.encrypt.data.transfer.cipher.suites"
"dfs.namenode.legacy-oiv-image.dir"
"dfs.provided.aliasmap.leveldb.path"
"dfs.encrypt.data.transfer.algorithm"
"dfs.ha.namenode.id"
"dfs.client.cache.drop.behind.reads"
"dfs.data.transfer.protection"
"dfs.metrics.session-id"
"dfs.namenode.rpc-address"
"dfs.nameservices"

2 parameters read from function getFloat:
"dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction" 0.75F
"dfs.namenode.available-space-block-placement-policy.balanced-space-preference-fraction" 0.6F

35 parameters read from function get2:
"dfs.disk.balancer.plan.valid.interval" "1d"
"dfs.provided.aliasmap.text.write.dir" $r4
"dfs.namenode.kerberos.principal" ""
"dfs.provided.storage.id" "DS-PROVIDED"
"dfs.journalnode.https-address" "0.0.0.0:8481"
"dfs.datanode.startup" $r3
"dfs.journalnode.rpc-address" "0.0.0.0:8485"
"dfs.mover.address" "0.0.0.0:0"
"dfs.datatransfer.client.variablewhitelist.file" r5
"dfs.client.context" "default"
"dfs.balancer.address" "0.0.0.0:0"
"dfs.datatransfer.server.variablewhitelist.file" "/etc/hadoop/whitelist"
"dfs.provided.aliasmap.text.read.file" "file:///tmp/blocks.csv"
"dfs.provided.aliasmap.text.delimiter" ","
"dfs.journalnode.http-address" "0.0.0.0:8480"
"dfs.hosts.exclude" ""
"dfs.user.home.dir.prefix" "/user"
"dfs.image.compression.codec" "org.apache.hadoop.io.compress.DefaultCodec"
"dfs.hosts" ""
"dfs.journalnode.edits.dir" "/tmp/hadoop/dfs/journalnode/"
"dfs.checksum.combine.mode" "MD5MD5CRC"
"dfs.datatransfer.client.fixedwhitelist.file" r4
"dfs.datatransfer.server.variableBlackList.file" "/etc/hadoop/blackList"
"dfs.datatransfer.server.fixedwhitelist.file" "/etc/hadoop/fixedwhitelist"
"dfs.cluster.administrators" " "
"dfs.datatransfer.client.fixedBlackList.file" r4
"dfs.datatransfer.client.variableBlackList.file" r5
"dfs.client.block.write.replace-datanode-on-failure.policy" "DEFAULT"
"dfs.datanode.data.dir.perm" "700"
"dfs.datanode.kerberos.principal" ""
"dfs.http.policy" $r3
"dfs.datatransfer.server.fixedBlackList.file" "/etc/hadoop/fixedBlackList"
"dfs.https.server.keystore.resource" "ssl-server.xml"
"dfs.checksum.type" "CRC32C"
"dfs.namenode.startup" $r2

