dfs.block.access.token.enable false
dfs.block.access.token.protobuf.enable false
dfs.block.local-path-access.user
dfs.block.placement.ec.classname $r11, class "Lorg/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicy;"
dfs.block.replicator.classname $r7, class "Lorg/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicy;"
dfs.blockreport.incremental.intervalMsec 0L
dfs.blockreport.initialDelay 0L, $r27
dfs.blockreport.intervalMsec 21600000L
dfs.blockreport.split.threshold 1000000L
dfs.block.scanner.volume.bytes.per.second 1048576L
dfs.blocksize 134217728L
dfs.bytes-per-checksum 512
dfs.cachereport.intervalMsec 10000L
dfs.checksum.combine.mode "MD5MD5CRC"
dfs.checksum.type "CRC32C"
dfs.client.block.write.locateFollowingBlock.initial.delay.ms 400
dfs.client.block.write.locateFollowingBlock.retries 5
dfs.client.block.write.replace-datanode-on-failure.best-effort false
dfs.client.block.write.replace-datanode-on-failure.enable true
dfs.client.block.write.replace-datanode-on-failure.min-replication 0
dfs.client.block.write.replace-datanode-on-failure.policy "DEFAULT"
dfs.client.block.write.retries 3
dfs.client.cached.conn.retry 3
dfs.client.cache.drop.behind.reads
dfs.client.cache.drop.behind.reads false
dfs.client.cache.drop.behind.writes
dfs.client.cache.drop.behind.writes false
dfs.client.cache.readahead
dfs.client.cache.readahead 0L
dfs.client.context "default"
dfs.client.datanode-restart.timeout 30L, $r7
dfs.client.domain.socket.data.traffic false
dfs.client.failover.max.attempts 15
dfs.client.failover.sleep.base.millis 500
dfs.client.failover.sleep.max.millis 15000
dfs.client.hedged.read.threadpool.size 0
dfs.client.hedged.read.threshold.millis 500L
dfs.client.https.need-auth false
dfs.client.key.provider.cache.expiry $l3
dfs.client.local.interfaces
dfs.client.max.block.acquire.failures 3
dfs.client.mmap.cache.size 256
dfs.client.mmap.cache.timeout.ms 3600000L
dfs.client.mmap.enabled true
dfs.client.mmap.retry.timeout.ms 300000L
dfs.client.read.prefetch.size $l23
dfs.client.read.shortcircuit.buffer.size 1048576
dfs.client.read.shortcircuit.metrics.sampling.percentage 0
dfs.client.read.short.circuit.replica.stale.threshold.ms 1800000L
dfs.client.read.shortcircuit.skip.checksum false
dfs.client.read.shortcircuit.streams.cache.expiry.ms 300000L
dfs.client.read.shortcircuit.streams.cache.size 256
dfs.client.read.striped.threadpool.size 18
dfs.client.replica.accessor.builder.classes
dfs.client.retry.interval-ms.get-last-block-length 4000
dfs.client.retry.max.attempts 10
dfs.client.retry.times.get-last-block-length 3
dfs.client.retry.window.base 3000
dfs.client.server-defaults.validity.period.ms $l7
dfs.client.slow.io.warning.threshold.ms 30000L
dfs.client.socketcache.capacity 16
dfs.client.socketcache.expiryMsec 3000L
dfs.client.socket.send.buffer.size 0
dfs.client.socket-timeout 60000
dfs.client.test.drop.namenode.response.number 0
dfs.client.use.datanode.hostname false
dfs.client.use.legacy.blockreader.local false
dfs.client.write.byte-array-manager.count-limit 2048
dfs.client.write.byte-array-manager.count-reset-time-period-ms 10000L
dfs.client.write.byte-array-manager.count-threshold 128
dfs.client.write.byte-array-manager.enabled false
dfs.client.write.exclude.nodes.cache.expiry.interval.millis 600000L
dfs.client.write.max-packets-in-flight 80
dfs.client-write-packet-size 65536
dfs.cluster.administrators " "
dfs.datanode.address "0.0.0.0:9866"
dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction 0.75F
dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold 10737418240L
dfs.datanode.balance.bandwidthPerSec 10485760L
dfs.datanode.balance.max.concurrent.moves 50
dfs.datanode.block.id.layout.upgrade.threads 12
dfs.datanode.block-pinning.enabled false
dfs.datanode.bp-ready.timeout 20L, $r49
dfs.datanode.cached-dfsused.check.interval.ms 600000L
dfs.datanode.cache.revocation.polling.ms 500L
dfs.datanode.cache.revocation.timeout.ms 900000L
dfs.datanode.data.dir
dfs.datanode.data.dir.perm "700"
dfs.datanode.directoryscan.interval 21600L, $r3
dfs.datanode.directoryscan.interval 21600L, $r8
dfs.datanode.directoryscan.threads 1
dfs.datanode.directoryscan.throttle.limit.ms.per.sec 1000
dfs.datanode.disk.check.min.gap "15m", $r7
dfs.datanode.disk.check.min.gap "15m", $r8
dfs.datanode.disk.check.timeout "10m", $r3
dfs.datanode.disk.check.timeout "10m", $r7
dfs.datanode.disk.check.timeout "10m", $r9
dfs.datanode.dns.interface
dfs.datanode.dns.nameserver
dfs.datanode.drop.cache.behind.reads false
dfs.datanode.drop.cache.behind.writes false
dfs.datanode.duplicate.replica.deletion true
dfs.datanode.du.reserved.calculator $r2, class "Lorg/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReservedSpaceCalculator;"
dfs.datanode.ec.reconstruction.stripedread.buffer.size 65536
dfs.datanode.ec.reconstruction.stripedread.timeout.millis 5000
dfs.datanode.ec.reconstruction.threads 8
dfs.datanode.ec.reconstruction.xmits.weight 0.5F
dfs.datanode.enable.fileio.fault.injection false
dfs.datanode.failed.volumes.tolerated 0
dfs.datanode.fileio.profiling.sampling.percentage 0
dfs.datanode.fsdatasetcache.max.threads.per.volume 4
dfs.datanode.fsdataset.factory class "Lorg/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetFactory;", class "Lorg/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi$Factory;"
dfs.datanode.fsdataset.volume.choosing.policy class "Lorg/apache/hadoop/hdfs/server/datanode/fsdataset/RoundRobinVolumeChoosingPolicy;", class "Lorg/apache/hadoop/hdfs/server/datanode/fsdataset/VolumeChoosingPolicy;"
dfs.datanode.handler.count 10
dfs.datanode.hostname
dfs.datanode.http.address "0.0.0.0:9864"
dfs.datanode.http.address null
dfs.datanode.http.internal-proxy.port 0
dfs.datanode.https.address "0.0.0.0:9865"
dfs.datanode.ipc.address
dfs.datanode.lazywriter.interval.sec 60
dfs.datanode.lifeline.interval.seconds $l20
dfs.datanode.max.locked.memory 0L
dfs.datanode.max.transfer.threads 4096
dfs.datanode.metrics.logger.period.seconds 600
dfs.datanode.min.supported.namenode.version "2.1.0-beta"
dfs.datanode.network.counts.cache.max.size 2147483647
dfs.datanode.non.local.lazy.persist false
dfs.datanode.oob.timeout-ms "1500,0,0,0"
dfs.datanode.outliers.report.interval "30m", $r21
dfs.datanode.parallel.volumes.load.threads.num i0
dfs.datanode.peer.stats.enabled false
dfs.datanode.plugins
dfs.datanode.plugins class "Lorg/apache/hadoop/util/ServicePlugin;"
dfs.datanode.ram.disk.replica.tracker $r4, class "Lorg/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskReplicaTracker;"
dfs.datanode.readahead.bytes 4194304L
dfs.datanode.restart.replica.expiration 50L
dfs.datanode.scan.period.hours 504L
dfs.datanode.shared.file.descriptor.paths
dfs.datanode.slow.io.warning.threshold.ms 300L
dfs.datanode.socket.reuse.keepalive 4000
dfs.datanode.socket.write.timeout 480000
dfs.datanode.startup $r3
dfs.datanode.sync.behind.writes false
dfs.datanode.sync.behind.writes.in.background false
dfs.datanode.synconclose false
dfs.datanode.transfer.socket.recv.buffer.size 0
dfs.datanode.transfer.socket.send.buffer.size 0
dfs.datanode.transferTo.allowed true
dfs.datanode.use.datanode.hostname false
dfs.datanode.volumes.replica-add.threadpool.size i1
dfs.datanode.xceiver.stop.timeout.millis 60000L
dfs.datatransfer.client.fixedBlackList.file r4
dfs.datatransfer.client.fixedwhitelist.file r4
dfs.data.transfer.client.tcpnodelay true
dfs.datatransfer.client.variableBlackList.cache.secs 3600L
dfs.datatransfer.client.variableBlackList.enable false
dfs.datatransfer.client.variableBlackList.file r5
dfs.datatransfer.client.variablewhitelist.cache.secs 3600L
dfs.datatransfer.client.variablewhitelist.enable false
dfs.datatransfer.client.variablewhitelist.file r5
dfs.data.transfer.protection
dfs.data.transfer.saslproperties.resolver.class r8, class "Lorg/apache/hadoop/security/SaslPropertiesResolver;"
dfs.datatransfer.server.fixedBlackList.file "/etc/hadoop/fixedBlackList"
dfs.datatransfer.server.fixedwhitelist.file "/etc/hadoop/fixedwhitelist"
dfs.data.transfer.server.tcpnodelay true
dfs.datatransfer.server.variableBlackList.cache.secs 3600L
dfs.datatransfer.server.variableBlackList.enable false
dfs.datatransfer.server.variableBlackList.file "/etc/hadoop/blackList"
dfs.datatransfer.server.variablewhitelist.cache.secs 3600L
dfs.datatransfer.server.variablewhitelist.enable false
dfs.datatransfer.server.variablewhitelist.file "/etc/hadoop/whitelist"
dfs.disk.balancer.block.tolerance.percent 10L
dfs.disk.balancer.enabled true
dfs.disk.balancer.max.disk.errors 5L
dfs.disk.balancer.max.disk.throughputInMBperSec 10
dfs.disk.balancer.max.disk.throughputInMBperSec 10L
dfs.disk.balancer.plan.valid.interval "1d"
dfs.disk.balancer.plan.valid.interval "1d", $r10
dfs.domain.socket.disable.interval.seconds 600L
dfs.domain.socket.path ""
dfs.edit.log.transfer.bandwidthPerSec 0L
dfs.edit.log.transfer.timeout 30000
dfs.encrypt.data.transfer.algorithm
dfs.encrypt.data.transfer.cipher.key.bitlength 128
dfs.encrypt.data.transfer.cipher.suites
dfs.encrypt.data.transfer false
dfs.federation.router.admin-bind-host "dfs.federation.router.admin-address", "0.0.0.0:8111", 8111
dfs.federation.router.admin-bind-host $r13
dfs.federation.router.admin.enable true
dfs.federation.router.admin.handler.count 1
dfs.federation.router.cache.ttl $l0, $r2
dfs.federation.router.client.reject.overload false
dfs.federation.router.client.retry.max.attempts 3
dfs.federation.router.client.thread-size 32
dfs.federation.router.connection.clean.ms $l7
dfs.federation.router.connection.creator.queue-size 100
dfs.federation.router.connection.pool.clean.ms $l3
dfs.federation.router.connection.pool-size 64
dfs.federation.router.dn-report.cache-expire $l3, $r19
dfs.federation.router.dn-report.time-out $l1, $r18
dfs.federation.router.file.resolver.client.class $r3, class "Lorg/apache/hadoop/hdfs/server/federation/resolver/FileSubclusterResolver;"
dfs.federation.router.handler.count 10
dfs.federation.router.handler.queue.size 100
dfs.federation.router.heartbeat.enable true
dfs.federation.router.heartbeat.interval $l0
dfs.federation.router.heartbeat-state.interval $l1, $r2
dfs.federation.router.http-bind-host "dfs.federation.router.http-address", "0.0.0.0:50071", 50071
dfs.federation.router.http.enable true
dfs.federation.router.https-bind-host "dfs.federation.router.https-address", "0.0.0.0:50072", 50072
dfs.federation.router.metrics.class $r50, class "Lorg/apache/hadoop/hdfs/server/federation/router/RouterRpcMonitor;"
dfs.federation.router.metrics.enable true
dfs.federation.router.monitor.localnamenode.enable true
dfs.federation.router.monitor.namenode
dfs.federation.router.namenode.resolver.client.class $r3, class "Lorg/apache/hadoop/hdfs/server/federation/resolver/ActiveNamenodeResolver;"
dfs.federation.router.quota-cache.update.interval 60000L, $r2
dfs.federation.router.quota.enable false
dfs.federation.router.reader.count 1
dfs.federation.router.reader.queue.size 100
dfs.federation.router.rpc-bind-host "dfs.federation.router.rpc-address", "0.0.0.0:8888", 8888
dfs.federation.router.rpc.enable true
dfs.federation.router.safemode.enable true
dfs.federation.router.safemode.expiration $l5, $r6
dfs.federation.router.safemode.extension $l2, $r3
dfs.federation.router.store.connection.test $l0
dfs.federation.router.store.driver.class $r4, class "Lorg/apache/hadoop/hdfs/server/federation/store/driver/StateStoreDriver;"
dfs.federation.router.store.driver.file.directory
dfs.federation.router.store.driver.fs.path
dfs.federation.router.store.driver.zk.parent-path "/hdfs-federation"
dfs.federation.router.store.enable true
dfs.federation.router.store.membership.expiration $l0
dfs.federation.router.store.router.expiration $l2, $r13
dfs.federation.router.store.serializer $r2, class "Lorg/apache/hadoop/hdfs/server/federation/store/driver/StateStoreSerializer;"
dfs.ha.automatic-failover.enabled false
dfs.ha.log-roll.period 120L, $r7
dfs.ha.log-roll.rpc.timeout 20000
dfs.ha.namenode.id
dfs.ha.tail-edits.in-progress false
dfs.ha.tail-edits.max-txns-per-lock 9223372036854775807L
dfs.ha.tail-edits.namenode-retries 3
dfs.ha.tail-edits.period 60L, $r9
dfs.ha.tail-edits.rolledits.timeout 60
dfs.ha.zkfc.nn.http.timeout.ms 20000
dfs.ha.zkfc.port 8019
dfs.heartbeat.interval 3L, $r29
dfs.heartbeat.interval 3L, $r32
dfs.heartbeat.interval 3L, $r5
dfs.heartbeat.interval r3, $r8
dfs.hosts ""
dfs.hosts.exclude ""
dfs.http.client.failover.max.attempts 15
dfs.http.client.failover.sleep.base.millis 500
dfs.http.client.failover.sleep.max.millis 15000
dfs.http.client.retry.max.attempts 10
dfs.http.policy $r3
dfs.https.server.keystore.resource "ssl-server.xml"
dfs.image.compress false
dfs.image.compression.codec "org.apache.hadoop.io.compress.DefaultCodec"
dfs.image.transfer.bandwidthPerSec 0L
dfs.image.transfer-bootstrap-standby.bandwidthPerSec 0L
dfs.image.transfer.chunksize 65536
dfs.image.transfer.timeout 60000
dfs.internal.nameservices
dfs.journalnode.edits.dir "/tmp/hadoop/dfs/journalnode/"
dfs.journalnode.enable.sync true
dfs.journalnode.sync.interval 120000L
dfs.lock.suppress.warning.interval 10000L, $r13
dfs.metrics.percentiles.intervals
dfs.metrics.session-id
dfs.namenode.available-space-block-placement-policy.balanced-space-preference-fraction 0.6F
dfs.namenode.block-placement-policy.default.prefer-local-node true
dfs.namenode.checkpoint.check.period 60L, $r2
dfs.namenode.checkpoint.check.quiet-multiplier 1.5
dfs.namenode.checkpoint.max-retries 3
dfs.namenode.checkpoint.period 3600L, $r3
dfs.namenode.checkpoint.txns 1000000L
dfs.namenode.ec.system.default.policy "RS-6-3-1024k"
dfs.namenode.edits.asynclogging true
dfs.namenode.edits.dir.minimum 1
dfs.namenode.edits.noeditlogchannelflush false
dfs.namenode.inotify.max.events.per.rpc 1000
dfs.namenode.kerberos.principal ""
dfs.namenode.legacy-oiv-image.dir
dfs.namenode.max.extra.edits.segments.retained 10000
dfs.namenode.max-num-blocks-to-log 1000L
dfs.namenode.name.dir.restore false
dfs.namenode.num.checkpoints.retained 2
dfs.namenode.num.extra.edits.retained 1000000L
dfs.namenode.redundancy.considerLoad.factor 2.0
dfs.namenode.redundancy.considerLoad true
dfs.namenode.replication.min 1
dfs.namenode.rpc-address
dfs.namenode.servicerpc-address
dfs.namenode.shared.edits.dir
dfs.namenode.stale.datanode.interval 30000L
dfs.namenode.startup $r2
dfs.namenode.tolerate.heartbeat.multiplier 4
dfs.namenode.upgrade.domain.factor 3
dfs.nameservice.id
dfs.nameservices
dfs.permissions.enabled true
dfs.permissions.superusergroup "supergroup"
dfs.pipeline.ecn false
dfs.provided.aliasmap.class class "Lorg/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap;", class "Lorg/apache/hadoop/hdfs/server/common/blockaliasmap/BlockAliasMap;"
dfs.provided.aliasmap.inmemory.batch-size 500
dfs.provided.aliasmap.inmemory.dnrpc-address "0.0.0.0:50200"
dfs.provided.aliasmap.leveldb.path
dfs.provided.aliasmap.load.retries 0
dfs.provided.aliasmap.text.codec
dfs.provided.aliasmap.text.delimiter ","
dfs.provided.aliasmap.text.read.file "file:///tmp/blocks.csv"
dfs.provided.aliasmap.text.write.dir $r4
dfs.provided.storage.id "DS-PROVIDED"
dfs.replication 3
dfs.short.circuit.shared.memory.watcher.interrupt.check.ms 60000
dfs.trustedchannel.resolver.class class "Lorg/apache/hadoop/hdfs/protocol/datatransfer/TrustedChannelResolver;", class "Lorg/apache/hadoop/hdfs/protocol/datatransfer/TrustedChannelResolver;"
dfs.user.home.dir.prefix "/user"
dfs.web.authentication.filter "org.apache.hadoop.hdfs.web.AuthFilter"
dfs.web.authentication.kerberos.keytab
dfs.web.authentication.kerberos.principal
dfs.web.authentication.simple.anonymous.allowed
dfs.webhdfs.acl.provider.permission.pattern "^(default:
dfs.webhdfs.netty.high.watermark 65535
dfs.webhdfs.netty.low.watermark 32768
dfs.webhdfs.oauth2.access.token.provider class "Lorg/apache/hadoop/hdfs/web/oauth2/ConfCredentialBasedAccessTokenProvider;", class "Lorg/apache/hadoop/hdfs/web/oauth2/AccessTokenProvider;"
dfs.webhdfs.oauth2.enabled false
dfs.webhdfs.rest-csrf.custom-header "X-XSRF-HEADER"
dfs.webhdfs.rest-csrf.enabled false
dfs.webhdfs.socket.connect-timeout 60000L, $r5
dfs.webhdfs.socket.read-timeout 60000L, $r6
dfs.webhdfs.ugi.expire.after.access 600000
dfs.webhdfs.user.provider.user.pattern "^[A-Za-z_][A-Za-z0-9._-]*[$]?$"
dfs.xframe.enabled true
dfs.xframe.value "SAMEORIGIN"
hadoop.hdfs.configuration.version "UNSPECIFIED"
